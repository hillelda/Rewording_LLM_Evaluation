{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Using Llama-3-8B-Instruct to answer MMLU questions"
      ],
      "metadata": {
        "id": "bayLRx9OMaOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOHSiUCRJ5vh"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "access_token='hf_access_token'"
      ],
      "metadata": {
        "id": "4y-bUm1LKBvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=access_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token=access_token)\n"
      ],
      "metadata": {
        "id": "-eVRT5OUKFgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(question, possible_answers):\n",
        "    prompt = f\"Answer the following question by selecting the most appropriate choice.\\nQuestion: {question}\\n\"\n",
        "    prompt += \"Choices:\\n\"\n",
        "    for idx, answer in enumerate(possible_answers):\n",
        "        prompt += f\"{idx}. {answer}\\n\"\n",
        "    prompt += \"Answer (return only the corresponding number):\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=3,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            temperature=0.01,\n",
        "            do_sample=False\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "5l2YwxtcKjE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded['dataset.csv']))"
      ],
      "metadata": {
        "id": "p9YTEnOnK1TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a progress bar to pandas apply\n",
        "tqdm.pandas()\n",
        "\n",
        "df['model_response'] = df.progress_apply(\n",
        "    lambda row: generate_response(create_prompt(row['prompt'], [row['0'], row['1'], row['2'], row['3']]))[-2],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "def check_correctness(model_response, correct_answer, possible_answers):\n",
        "    if model_response == \"\\n\":\n",
        "      return 0\n",
        "    try:\n",
        "      model_response = int(model_response)\n",
        "    except:\n",
        "      return 0\n",
        "    correct_answer = int(correct_answer)\n",
        "    # Check if the model's response is one of the possible answers\n",
        "    return int(model_response in possible_answers and model_response == correct_answer)\n",
        "\n",
        "df['is_correct'] = df.apply(lambda row: check_correctness(row['model_response'], row['answer'], [0,1,2,3]), axis=1)\n",
        "accuracy = df['is_correct'].mean()\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "output_csv_path = \"dataset_results.csv\"\n",
        "df.to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "id": "pAFKZLUQKn39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Llama Rephrases"
      ],
      "metadata": {
        "id": "vDYxmCNoLjt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=300,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            temperature=0.01,\n",
        "            do_sample=False\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "\n",
        "def generate_new_prompt(question, possible_answers, idx):\n",
        "    dict_idx_to_do = {\"4\": \"question\", \"0\": \"first answer\", \"1\": \"second answer\", \"2\": \"third answer\", \"3\": \"fourth answer\"}\n",
        "    prompt = f\"You are given the following multichoice question with the following answers, please rephrase the {dict_idx_to_do[idx]}. Make sure your answer is between 50-250 charachters.\\nQuestion: {question}\\n\"\n",
        "    prompt += \"Choices:\\n\"\n",
        "    for idx2, answer in enumerate(possible_answers):\n",
        "        prompt += f\"{idx2}. {answer}\\n\"\n",
        "    return prompt\n",
        "\n",
        "def get_generations():\n",
        "    df['new_question'] = df.progress_apply(\n",
        "    lambda row: generate_response(generate_new_prompt(row['prompt'], [row['0'], row['1'], row['2'], row['3']], 4)),\n",
        "    axis=1\n",
        ")\n",
        "    df['0_new'] = df.progress_apply(\n",
        "    lambda row: generate_response(generate_new_prompt(row['prompt'], [row['0'], row['1'], row['2'], row['3']], 0)),\n",
        "    axis=1\n",
        ")\n",
        "    df['1_new'] = df.progress_apply(\n",
        "    lambda row: generate_response(generate_new_prompt(row['prompt'], [row['0'], row['1'], row['2'], row['3']], 1)),\n",
        "    axis=1\n",
        ")\n",
        "    df['2_new'] = df.progress_apply(\n",
        "    lambda row: generate_response(generate_new_prompt(row['prompt'], [row['0'], row['1'], row['2'], row['3']], 2)),\n",
        "    axis=1\n",
        ")\n",
        "    df['3_new'] = df.progress_apply(\n",
        "    lambda row: generate_response(generate_new_prompt(row['prompt'], [row['0'], row['1'], row['2'], row['3']], 3)),\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "bWDkL1k8LEg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_generations()"
      ],
      "metadata": {
        "id": "gNXWKApyMMUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_csv_path = \"llama_generations_dataset.csv\"\n",
        "df.to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "id": "cEDtE0UiMQiV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
